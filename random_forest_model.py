import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import numpy as np

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
energy_data_train = pd.read_csv('data/monthly_consumption.csv', sep=',')
building_data = pd.read_csv('data/power-laws-forecasting-energy-consumption-metadata.csv', sep=';')
temperature_data = pd.read_csv('data/monthly_weather.csv', sep=',')
holidays_data = pd.read_csv('data/monthly_holidays.csv', sep=',')

# –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
data_train = pd.merge(energy_data_train, building_data, on='SiteId')
data_train = pd.merge(data_train, temperature_data, on=['SiteId', 'Month'])
data_train = pd.merge(data_train, holidays_data, on=['SiteId', 'Month'])

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Timestamp —É datetime
data_train['Timestamp'] = pd.to_datetime(data_train['Month'])

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
data_train['Month'] = data_train['Timestamp'].dt.month
data_train['Year'] = data_train['Timestamp'].dt.year

# –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ (—Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –º–∏–Ω—É–ª—ñ 3 –º—ñ—Å—è—Ü—ñ)
data_train['value_lag1'] = data_train.groupby('SiteId')['value'].shift(1)
data_train['value_lag2'] = data_train.groupby('SiteId')['value'].shift(2)
data_train['value_lag3'] = data_train.groupby('SiteId')['value'].shift(3)
data_train['value_rolling_mean'] = data_train.groupby('SiteId')['value'].rolling(window=3).mean().reset_index(level=0, drop=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
data_train.dropna(inplace=True)

# –û–±—Ä–æ–±–∫–∞ –≤–∏–∫–∏–¥—ñ–≤ (–∑–∞–º—ñ–Ω–∞ –Ω–∞ –º–µ–¥—ñ–∞–Ω—É)
Q1 = data_train['value'].quantile(0.25)
Q3 = data_train['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_train['value'] = np.where(
    (data_train['value'] < lower_bound) | (data_train['value'] > upper_bound),
    data_train['value'].median(),
    data_train['value']
)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
data_train = data_train.drop(columns=['obs_id', 'Timestamp'])

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_train.isnull().sum()

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–∏—à–µ —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
numerical_features = data_train.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
data_train[numerical_features] = scaler.fit_transform(data_train[numerical_features])

# –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_train.fillna(data_train.median(), inplace=True)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω–∞–≤—á–∞–ª—å–Ω—É —Ç–∞ —Ç–µ—Å—Ç–æ–≤—É –≤–∏–±—ñ—Ä–∫–∏ (–¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ)
X_train = data_train.drop(columns=["value"])  # –û–∑–Ω–∞–∫–∏
y_train = data_train["value"]  # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
energy_data_test = pd.read_csv('data/monthly_consumption_test.csv', sep=',')

# –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
data_test = pd.merge(energy_data_test, building_data, on='SiteId')
data_test = pd.merge(data_test, temperature_data, on=['SiteId', 'Month'])
data_test = pd.merge(data_test, holidays_data, on=['SiteId', 'Month'])

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Timestamp —É datetime
data_test['Timestamp'] = pd.to_datetime(data_test['Month'])

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
data_test['Month'] = data_test['Timestamp'].dt.month
data_test['Year'] = data_test['Timestamp'].dt.year

# –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ (—Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –º–∏–Ω—É–ª—ñ 3 –º—ñ—Å—è—Ü—ñ)
data_test['value_lag1'] = data_test.groupby('SiteId')['value'].shift(1)
data_test['value_lag2'] = data_test.groupby('SiteId')['value'].shift(2)
data_test['value_lag3'] = data_test.groupby('SiteId')['value'].shift(3)
data_test['value_rolling_mean'] = data_test.groupby('SiteId')['value'].rolling(window=3).mean().reset_index(level=0, drop=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
data_test.dropna(inplace=True)

# –û–±—Ä–æ–±–∫–∞ –≤–∏–∫–∏–¥—ñ–≤ (–∑–∞–º—ñ–Ω–∞ –Ω–∞ –º–µ–¥—ñ–∞–Ω—É)
Q1 = data_test['value'].quantile(0.25)
Q3 = data_test['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_test['value'] = np.where(
    (data_test['value'] < lower_bound) | (data_test['value'] > upper_bound),
    data_test['value'].median(),
    data_test['value']
)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
data_test = data_test.drop(columns=['obs_id', 'Timestamp'])

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_test.isnull().sum()

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ç–æ–≥–æ —Å–∞–º–æ–≥–æ scaler
data_test[numerical_features] = scaler.transform(data_test[numerical_features])

# –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_test.fillna(data_test.median(), inplace=True)

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
X_test = data_test.drop(columns=["value"])  # –û–∑–Ω–∞–∫–∏
y_test = data_test["value"]  # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞

# –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª—ñ Random Forest
model = RandomForestRegressor(random_state=42)

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è —Ç—é–Ω—ñ–Ω–≥—É
param_dist = {
    'n_estimators': [100, 200, 300],  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
    'max_depth': [3, 6, 9, None],     # –ì–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤
    'min_samples_split': [2, 5, 10],  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    'min_samples_leaf': [1, 2, 4],    # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç—ñ
    'max_features': ['auto', 'sqrt']  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –¥–ª—è —Ä–æ–∑–≥–ª—è–¥—É –ø—ñ–¥ —á–∞—Å —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
}

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è RandomizedSearchCV –¥–ª—è –ø–æ—à—É–∫—É –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=20,  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π
    cv=3,
    scoring='neg_mean_absolute_error',
    random_state=42,
    n_jobs=-1
)
random_search.fit(X_train, y_train)
print("–ù–∞–π–∫—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:", random_search.best_params_)

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
best_model = random_search.best_estimator_

# –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ
y_pred = best_model.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ
mae = mean_absolute_error(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)

print(f"üìâ Mean Absolute Error (MAE): {mae:.2f}")
print(f"üìä Mean Absolute Percentage Error (MAPE): {mape * 100:.2f}%")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
plt.figure(figsize=(10, 6))
plt.plot(y_test.values, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.legend()
plt.title("Actual vs Predicted (Test Dataset)")
plt.show()