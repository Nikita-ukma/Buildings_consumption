import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
energy_data_train = pd.read_csv('data/monthly_consumption.csv', sep=',')
building_data = pd.read_csv('data/power-laws-forecasting-energy-consumption-metadata.csv', sep=';')
temperature_data = pd.read_csv('data/monthly_weather.csv', sep=',')
holidays_data = pd.read_csv('data/monthly_holidays.csv', sep=',')

# –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
data_train = pd.merge(energy_data_train, building_data, on='SiteId')
data_train = pd.merge(data_train, temperature_data, on=['SiteId', 'Month'])
data_train = pd.merge(data_train, holidays_data, on=['SiteId', 'Month'])

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Timestamp —É datetime
data_train['Timestamp'] = pd.to_datetime(data_train['Month'])

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
data_train['Month'] = data_train['Timestamp'].dt.month
data_train['Year'] = data_train['Timestamp'].dt.year

# –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ (—Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –º–∏–Ω—É–ª—ñ 3 –º—ñ—Å—è—Ü—ñ)
data_train['value_lag1'] = data_train.groupby('SiteId')['value'].shift(1)
data_train['value_lag2'] = data_train.groupby('SiteId')['value'].shift(2)
data_train['value_lag3'] = data_train.groupby('SiteId')['value'].shift(3)
data_train['value_rolling_mean'] = data_train.groupby('SiteId')['value'].rolling(window=3).mean().reset_index(level=0, drop=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
data_train.dropna(inplace=True)

# –û–±—Ä–æ–±–∫–∞ –≤–∏–∫–∏–¥—ñ–≤ (–∑–∞–º—ñ–Ω–∞ –Ω–∞ –º–µ–¥—ñ–∞–Ω—É)
Q1 = data_train['value'].quantile(0.25)
Q3 = data_train['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_train['value'] = np.where(
    (data_train['value'] < lower_bound) | (data_train['value'] > upper_bound),
    data_train['value'].median(),
    data_train['value']
)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
data_train = data_train.drop(columns=['obs_id', 'Timestamp'])

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_train.isnull().sum()

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–∏—à–µ —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
numerical_features = data_train.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
data_train[numerical_features] = scaler.fit_transform(data_train[numerical_features])

# –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_train.fillna(data_train.median(), inplace=True)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω–∞–≤—á–∞–ª—å–Ω—É —Ç–∞ —Ç–µ—Å—Ç–æ–≤—É –≤–∏–±—ñ—Ä–∫–∏ (–¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ)
X_train = data_train.drop(columns=["value"])  # –û–∑–Ω–∞–∫–∏
y_train = data_train["value"]  # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
energy_data_test = pd.read_csv('data/monthly_consumption_test.csv', sep=',')

# –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
data_test = pd.merge(energy_data_test, building_data, on='SiteId')
data_test = pd.merge(data_test, temperature_data, on=['SiteId', 'Month'])
data_test = pd.merge(data_test, holidays_data, on=['SiteId', 'Month'])

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Timestamp —É datetime
data_test['Timestamp'] = pd.to_datetime(data_test['Month'])

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
data_test['Month'] = data_test['Timestamp'].dt.month
data_test['Year'] = data_test['Timestamp'].dt.year

# –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ (—Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –º–∏–Ω—É–ª—ñ 3 –º—ñ—Å—è—Ü—ñ)
data_test['value_lag1'] = data_test.groupby('SiteId')['value'].shift(1)
data_test['value_lag2'] = data_test.groupby('SiteId')['value'].shift(2)
data_test['value_lag3'] = data_test.groupby('SiteId')['value'].shift(3)
data_test['value_rolling_mean'] = data_test.groupby('SiteId')['value'].rolling(window=3).mean().reset_index(level=0, drop=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
data_test.dropna(inplace=True)

# –û–±—Ä–æ–±–∫–∞ –≤–∏–∫–∏–¥—ñ–≤ (–∑–∞–º—ñ–Ω–∞ –Ω–∞ –º–µ–¥—ñ–∞–Ω—É)
Q1 = data_test['value'].quantile(0.25)
Q3 = data_test['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_test['value'] = np.where(
    (data_test['value'] < lower_bound) | (data_test['value'] > upper_bound),
    data_test['value'].median(),
    data_test['value']
)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
data_test = data_test.drop(columns=['obs_id', 'Timestamp'])

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_test.isnull().sum()

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ç–æ–≥–æ —Å–∞–º–æ–≥–æ scaler
data_test[numerical_features] = scaler.transform(data_test[numerical_features])

# –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_test.fillna(data_test.median(), inplace=True)

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
X_test = data_test.drop(columns=["value"])  # –û–∑–Ω–∞–∫–∏
y_test = data_test["value"]  # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —É —Ñ–æ—Ä–º–∞—Ç, –ø—Ä–∏–¥–∞—Ç–Ω–∏–π –¥–ª—è GRU
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data.iloc[i:(i + seq_length)].values
        y = data.iloc[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

seq_length = 12  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 12 –º—ñ—Å—è—Ü—ñ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
X_train_seq, y_train_seq = create_sequences(X_train, seq_length)
X_test_seq, y_test_seq = create_sequences(X_test, seq_length)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö
print("–¢–∏–ø–∏ –¥–∞–Ω–∏—Ö —É X_train_seq:", X_train_seq.dtype)
print("–¢–∏–ø–∏ –¥–∞–Ω–∏—Ö —É y_train_seq:", y_train_seq.dtype)

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —É float32
X_train_seq = X_train_seq.astype(np.float32)
y_train_seq = y_train_seq.astype(np.float32)
X_test_seq = X_test_seq.astype(np.float32)
y_test_seq = y_test_seq.astype(np.float32)

# –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª—ñ GRU
model = Sequential()
model.add(GRU(100, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))  # –ó–±—ñ–ª—å—à–µ–Ω–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω—ñ–≤
model.add(Dropout(0.2))
model.add(Dense(1))

# –ö–æ–º–ø—ñ–ª—è—Ü—ñ—è –º–æ–¥–µ–ª—ñ
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
history = model.fit(
    X_train_seq, y_train_seq,
    epochs=100,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
    batch_size=32,
    validation_data=(X_test_seq, y_test_seq),
    verbose=1
)

# –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ
y_pred = model.predict(X_test_seq)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–æ—Ä–º
print("–§–æ—Ä–º–∞ y_test_seq:", y_test_seq.shape)
print("–§–æ—Ä–º–∞ y_pred:", y_pred.shape)

# –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ
mae = mean_absolute_error(y_test_seq, y_pred)
mape = mean_absolute_percentage_error(y_test_seq, y_pred)

print(f"üìâ Mean Absolute Error (MAE): {mae:.2f}")
print(f"üìä Mean Absolute Percentage Error (MAPE): {mape * 100:.2f}%")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
plt.figure(figsize=(10, 6))
plt.plot(y_test_seq, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.legend()
plt.title("Actual vs Predicted (Test Dataset)")
plt.show()