import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
energy_data_train = pd.read_csv('data/monthly_consumption.csv', sep=',')
# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
print(energy_data_train.shape)  # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É

building_data = pd.read_csv('data/power-laws-forecasting-energy-consumption-metadata.csv', sep=';')
print(building_data.shape)  # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É
temperature_data = pd.read_csv('data/monthly_weather.csv', sep=',')
print(temperature_data.shape)  # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É

holidays_data = pd.read_csv('data/monthly_holidays.csv', sep=',')
print(holidays_data.shape)  # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É



# –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
data_train = pd.merge(energy_data_train, building_data, on='SiteId')
data_train = pd.merge(data_train, temperature_data, on=['SiteId', 'Month'])
data_train = pd.merge(data_train, holidays_data, on=['SiteId', 'Month'])

print(f"energy_data_train: {energy_data_train.shape}")
print(f"building_data: {building_data.shape}")
print(f"temperature_data: {temperature_data.shape}")
print(f"holidays_data: {holidays_data.shape}")

print(f"After merging: {data_train.shape}")  # –ü—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å


# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Timestamp —É datetime —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –π–æ–≥–æ —è–∫ —ñ–Ω–¥–µ–∫—Å—É
data_train['Timestamp'] = pd.to_datetime(data_train['Month'])
data_train.set_index('Timestamp', inplace=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —É —á–∞—Å–æ–≤–æ–º—É —ñ–Ω–¥–µ–∫—Å—ñ
data_train = data_train[~data_train.index.duplicated(keep='first')]

# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —á–∞—Å—Ç–æ—Ç–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å—É (–º—ñ—Å—è—á–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞)
data_train = data_train.asfreq('MS')  # 'MS' –¥–ª—è –º—ñ—Å—è—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É


print(f"After merging: {data_train.shape}")  # –ü—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å
# –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
data_train['value_lag1'] = data_train.groupby('SiteId')['value'].shift(1)
data_train['value_lag2'] = data_train.groupby('SiteId')['value'].shift(2)
data_train['value_lag3'] = data_train.groupby('SiteId')['value'].shift(3)
data_train['value_rolling_mean'] = data_train.groupby('SiteId')['value'].rolling(window=3).mean().reset_index(level=0, drop=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
data_train.dropna(inplace=True)

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
y_train = data_train["value"]

# # –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ ARIMA (—Ä—É—á–Ω–∏–π –ø—ñ–¥–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)
# model = ARIMA(y_train, order=(5, 1, 0))  # –ü—Ä–∏–∫–ª–∞–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (p, d, q)
# model_fit = model.fit()

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
energy_data_test = pd.read_csv('data/monthly_consumption_test.csv', sep=',')

# –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
data_test = pd.merge(energy_data_test, building_data, on='SiteId')
data_test = pd.merge(data_test, temperature_data, on=['SiteId', 'Month'])
data_test = pd.merge(data_test, holidays_data, on=['SiteId', 'Month'])

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Timestamp —É datetime —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –π–æ–≥–æ —è–∫ —ñ–Ω–¥–µ–∫—Å—É
data_test['Timestamp'] = pd.to_datetime(data_test['Month'])
data_test.set_index('Timestamp', inplace=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —É —á–∞—Å–æ–≤–æ–º—É —ñ–Ω–¥–µ–∫—Å—ñ
data_test = data_test[~data_test.index.duplicated(keep='first')]

# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —á–∞—Å—Ç–æ—Ç–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å—É (–º—ñ—Å—è—á–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞)
data_test = data_test.asfreq('MS')  # 'MS' –¥–ª—è –º—ñ—Å—è—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —ñ–Ω–¥–µ–∫—Å –º–æ–Ω–æ—Ç–æ–Ω–Ω–∏–π
if not data_test.index.is_monotonic_increasing:
    data_test = data_test.sort_index()  # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —ñ–Ω–¥–µ–∫—Å–æ–º, —è–∫—â–æ –≤—ñ–Ω –Ω–µ –º–æ–Ω–æ—Ç–æ–Ω–Ω–∏–π

# –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ (—Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –º–∏–Ω—É–ª—ñ 3 –º—ñ—Å—è—Ü—ñ)
data_test['value_lag1'] = data_test.groupby('SiteId')['value'].shift(1)
data_test['value_lag2'] = data_test.groupby('SiteId')['value'].shift(2)
data_test['value_lag3'] = data_test.groupby('SiteId')['value'].shift(3)
data_test['value_rolling_mean'] = data_test.groupby('SiteId')['value'].rolling(window=3).mean().reset_index(level=0, drop=True)

# –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
data_test.dropna(inplace=True)

# –û–±—Ä–æ–±–∫–∞ –≤–∏–∫–∏–¥—ñ–≤ (–∑–∞–º—ñ–Ω–∞ –Ω–∞ –º–µ–¥—ñ–∞–Ω—É)
Q1 = data_test['value'].quantile(0.25)
Q3 = data_test['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_test['value'] = np.where(
    (data_test['value'] < lower_bound) | (data_test['value'] > upper_bound),
    data_test['value'].median(),
    data_test['value']
)

# # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
# y_test = data_test["value"]  # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞
# print(data_train.shape)  # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É
# # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ
# y_pred = model_fit.forecast(steps=len(y_test))

# # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
# mae = mean_absolute_error(y_test, y_pred)
# mape = mean_absolute_percentage_error(y_test, y_pred)
# print(data_train.shape)  # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É
# print(f"üìâ Mean Absolute Error (MAE): {mae:.2f}")
# print(f"üìä Mean Absolute Percentage Error (MAPE): {mape * 100:.2f}%")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
print(data_train.shape)  # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É

# plt.figure(figsize=(12, 6))
# plt.plot(y_test.index, y_test.values, label='Actual')
# plt.plot(y_test.index, y_pred, label='Predicted')
# plt.legend()
# plt.title("Actual vs Predicted (ARIMA)")
# plt.show()
