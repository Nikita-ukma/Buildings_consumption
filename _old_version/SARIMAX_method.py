import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
import warnings

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
pd.options.mode.chained_assignment = None  # –í–∏–º–∫–Ω–µ–Ω–Ω—è SettingWithCopyWarning

def clean_data(df):
    """–§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –≤—ñ–¥ inf —ñ NaN"""
    df = df.replace([np.inf, -np.inf], np.nan)
    
    # –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –¥–ª—è —á–∏—Å–ª–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if df[col].isna().any():
            df[col].fillna(df[col].median(), inplace=True)
    
    return df

def load_and_preprocess_data(train_test='train'):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö"""
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    energy_data = pd.read_csv(f'data/monthly_consumption{"_test" if train_test=="test" else ""}.csv', sep=',')
    building_data = pd.read_csv('data/power-laws-forecasting-energy-consumption-metadata.csv', sep=';')
    temperature_data = pd.read_csv('data/monthly_weather.csv', sep=',')
    holidays_data = pd.read_csv('data/monthly_holidays.csv', sep=',')
    
    # –û–±'—î–¥–Ω–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
    data = energy_data.merge(building_data, on='SiteId', how='left')
    data = data.merge(temperature_data, on=['SiteId', 'Month'], how='left')
    data = data.merge(holidays_data, on=['SiteId', 'Month'], how='left')
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –¥–∞—Ç–∏ —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É
    data['Timestamp'] = pd.to_datetime(data['Month'])
    data.set_index(['SiteId', 'Timestamp'], inplace=True)
    data = data.sort_index()
    
    # –û—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    data = clean_data(data)
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è –ª–∞–≥—ñ–≤ —Ç–∞ –∫–æ–≤–∑–∞—é—á–æ–≥–æ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ
    for site_id in data.index.get_level_values('SiteId').unique():
        site_mask = data.index.get_level_values('SiteId') == site_id
        data.loc[site_mask, 'value_lag1'] = data.loc[site_mask, 'value'].shift(1)
        data.loc[site_mask, 'value_lag2'] = data.loc[site_mask, 'value'].shift(2)
        data.loc[site_mask, 'value_lag3'] = data.loc[site_mask, 'value'].shift(3)
        data.loc[site_mask, 'value_rolling_mean'] = data.loc[site_mask, 'value'].rolling(3, min_periods=1).mean()
    
    # –ü–æ–≤—Ç–æ—Ä–Ω–µ –æ—á–∏—â–µ–Ω–Ω—è –ø—ñ—Å–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
    data = clean_data(data)
    
    return data

def train_and_predict(train_site, test_site):
    """–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–±'—î–∫—Ç–∞"""
    try:
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –µ–∫–∑–æ–≥–µ–Ω–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
        exog_features = ['average_temperature', 'number_of_holidays', 
                        'Surface', 'Sampling', 'BaseTemperature',
                        'value_lag1', 'value_lag2', 'value_lag3',
                        'value_rolling_mean']
        
        # –í–∏–±—ñ—Ä –ª–∏—à–µ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö —Ç–∞ –∫–æ—Ä–µ–∫—Ç–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
        available_features = [f for f in exog_features if f in train_site.columns]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
        if len(train_site) < 12 or len(test_site) == 0:
            return None
        
        # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            model = SARIMAX(train_site['value'],
                          exog=train_site[available_features] if available_features else None,
                          order=(1, 1, 1),
                          seasonal_order=(1, 1, 1, 12),
                          enforce_stationarity=False,
                          enforce_invertibility=False)
            
            model_fit = model.fit(disp=False)
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        if available_features:
            # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å inf/nan —É —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            if not np.isfinite(test_site[available_features].values).all():
                return None
                
            forecast = model_fit.get_forecast(steps=len(test_site), 
                                            exog=test_site[available_features])
        else:
            forecast = model_fit.get_forecast(steps=len(test_site))
        
        # –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        test_site['prediction'] = forecast.predicted_mean.values
        return test_site[['value', 'prediction']]
    
    except Exception as e:
        return None

# –û—Å–Ω–æ–≤–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–∏
data_train = load_and_preprocess_data('train')
data_test = load_and_preprocess_data('test')

results = []
for site_id in data_train.index.get_level_values('SiteId').unique():
    try:
        train_site = data_train.xs(site_id, level='SiteId').copy()
        test_site = data_test.xs(site_id, level='SiteId').copy()
        
        result = train_and_predict(train_site, test_site)
        if result is not None:
            results.append(result)
    
    except Exception as e:
        continue

# –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
if results:
    full_results = pd.concat(results)
    
    # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å —É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö (–Ω–∞ –≤–∏–ø–∞–¥–æ–∫, —è–∫—â–æ —â–æ—Å—å –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏)
    full_results = full_results.replace([np.inf, -np.inf], np.nan).dropna()
    
    if len(full_results) > 0:
        mae = mean_absolute_error(full_results['value'], full_results['prediction'])
        mape = mean_absolute_percentage_error(full_results['value'], full_results['prediction'])
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è {len(results)} –æ–±'—î–∫—Ç—ñ–≤:")
        print(f"MAE: {mae:.2f}")
        print(f"MAPE: {mape*100:.2f}%")
        
        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
        plt.figure(figsize=(12, 6))
        for site_id in full_results.index.get_level_values('SiteId').unique()[:10]:  # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ
            site_data = full_results.xs(site_id, level='SiteId')
            plt.plot(site_data.index, site_data['value'], label=f'Site {site_id} - Actual')
            plt.plot(site_data.index, site_data['prediction'], '--', label=f'Site {site_id} - Predicted')
        
        plt.title("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—É –∑ —Ñ–∞–∫—Ç–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∞–º–∏ (–ø–µ—Ä—à—ñ 10 –æ–±'—î–∫—Ç—ñ–≤)")
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()
    else:
        print("–ù–µ–º–∞—î –∫–æ—Ä–µ–∫—Ç–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –æ—Ü—ñ–Ω–∫–∏")
else:
    print("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –∂–æ–¥–Ω–æ–≥–æ –æ–±'—î–∫—Ç–∞")